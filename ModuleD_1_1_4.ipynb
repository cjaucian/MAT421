{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi7J5pivVvS59Pk5RymNS2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjaucian/MAT421/blob/main/ModuleD_1_1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.1 Introduction**"
      ],
      "metadata": {
        "id": "r--EGevbiYO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear algebra concepts play an important role in data science and machine learning algorithms. Basic concepts like vector spaces, orthogonality, eigenvalues, and matrix decomposition make up the process for solving problems in data science."
      ],
      "metadata": {
        "id": "02xLXW3_idRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.2 Elements of Linear Algebra**"
      ],
      "metadata": {
        "id": "v0fgwwQdkO2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Spaces**\n",
        "\n",
        "A new vector which is created by taking an initial subset and multiplying a constant then adding the results is called a linear combination. A result of the linear comination is a linear subspace.\n",
        "\n",
        "Definition: A linear subspace, V, is a subset of U that remains true under vector addition and scalar multiplication. For example, if u1 and u2 are included in set U. Additionally, alpha is included in the real number set. Then it must be true that:\n",
        "u1 + u2 are included in set U, and a*u1 is included in set U. Thus, the sum of a*un is defined as a linear combination.\n",
        "\n",
        "Sets can be defined as linearly independent or linearly dependent depending on whether the objects in the set can be shown as a linear combination of the other objects within the same set. If no other linear combinations are created, then the set is consided linearly independent. If this is not true, the set is linearly dependent.\n",
        "\n",
        "**Example**  Given the row vectors v = [1, 2, 3], w = [2, 2, 5], and u = [0, -3, 0], write the vector x = [-4, -8, -9]."
      ],
      "metadata": {
        "id": "OyjcPGRkkS65"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Qs6EqOZMsiny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52dd334-6293-4bd8-ffa1-f7ce222a5094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0. -0.  0.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[1, 2, 3], [2, 2, 5], [0, -3, 0]])\n",
        "y = np.array([0, 0, 0])\n",
        "x = np.linalg.solve(A, y)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By taking the row vectors and setting them equal to the zero vector, we can determine if they are linearly independent. Since we received a vector of [0, 0, 0], we can determine that they are linearly independent."
      ],
      "metadata": {
        "id": "Vka1hgiFxsX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Orthogonality**\n",
        "\n",
        "Orthonormal vectors are a set of vectors that sum to the zero vector and also has a set with a magnitude of 1.\n",
        "\n",
        "An optimized combination is not always possible, so the Best Approximation Theorem finds a solution that optimizes the combination of variables that gets closest to orthonormal basis.\n",
        "\n",
        "**Eigenvalues and Eigenvectors**\n",
        "\n",
        "A linear transformation of x, which is Ax will have a special transform:\n",
        "A * x = lambda * x\n",
        "\n",
        "In this case, A is a square matrix of dimensions n x n and x is a column vector of n x 1. For any lambda that satisfies the special transform equation above, is called an eigenvalue of A. The vector, x, is called the eigenvector that relates to the eigenvalue, lambda."
      ],
      "metadata": {
        "id": "9AvM9fwOyWPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example** Use numpy.linalg in Python to find the eigenvalue and eigenvector for given matrix and vector."
      ],
      "metadata": {
        "id": "6pPItKaQ5k4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the eigenvalues and eigenvectors for maxtrix A = [0,2],[2,3]."
      ],
      "metadata": {
        "id": "ALUHwldp8E20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import eig\n",
        "\n",
        "A = np.array([[0,2],\n",
        "              [2,3]])\n",
        "\n",
        "w,v = eig(A)\n",
        "\n",
        "print(\"Eigenvalue: \", w)\n",
        "print(\"Eigenvector\", v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1bsDFkd57mA",
        "outputId": "d2d85192-a0df-4024-f681-752192e15067"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalue:  [-1.  4.]\n",
            "Eigenvector [[-0.89442719 -0.4472136 ]\n",
            " [ 0.4472136  -0.89442719]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the eigenvalues and eigenvectors for matrix A = [1 1 4], [1 4 5], [1 2 3]."
      ],
      "metadata": {
        "id": "Vlkku_si8SpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[1,1,4],\n",
        "              [1,4,5],\n",
        "              [1,2,3]])\n",
        "\n",
        "w,v = eig(A)\n",
        "print(\"Eigenvalue: \", w)\n",
        "print(\"Eigenvector: \", v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOUFBld48CoT",
        "outputId": "89a1aaa7-52e6-4b2f-bce9-4ad995cbc959"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalue:  [ 7.38502039 -0.49012347  1.10510308]\n",
            "Eigenvector:  [[ 0.40856011  0.88957079  0.81314813]\n",
            " [ 0.79166661  0.23684039 -0.55929255]\n",
            " [ 0.4542495  -0.39060267  0.16118923]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.3 Linear Regression**"
      ],
      "metadata": {
        "id": "Z8tHOfST3qLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple way to model practical applications is linear regression. This is an approximation between unknown parameters which has been modeled by a simple linear equation.\n",
        "\n",
        "QR Decomposition is a method for solving least squares regression models. We take a matrix A and decompose it into two matrices Q and R. Q is an orthogonal matrix and R is special matrix called an upper triangular matrix. An upper triangular matrix has a diagonal boundary in which the values below the boundary are all zeros. The diagonal boundary are the the eigenvalues of the matrix."
      ],
      "metadata": {
        "id": "SwGygRWE3wPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example** Use the qr function in numply.linalg to decompose matrix A = [0,4],[5,6]. Then verify the results."
      ],
      "metadata": {
        "id": "N6Wb0mwe_WSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import qr\n",
        "\n",
        "A = np.array([[0, 4],[5, 6]])\n",
        "\n",
        "q,r = qr(A)\n",
        "\n",
        "print(\"Q: \", q)\n",
        "print(\"R: \", r)\n",
        "\n",
        "b = np.dot(q, r)\n",
        "print(\"QR: \", b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O4hst5U_gX0",
        "outputId": "d96b0eb0-ae91-4c81-b2b3-c9f539596955"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q:  [[ 0. -1.]\n",
            " [-1.  0.]]\n",
            "R:  [[-5. -6.]\n",
            " [ 0. -4.]]\n",
            "QR:  [[0. 4.]\n",
            " [5. 6.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In problems where we are trying to solve the system Ax = b, we can use the Least-squares Problem since the solution is often inconsistent. In this system, A is a n x m matrix and be is a n x 1 vector. By using an approximation of Ax, we can approximate b. If the matrix A is a square matrix, meaning that n = m then we can simply use the matrix inverse to find the solution. But we use the Lease-squares Problem assuming that n > m. Since n > m, is not a square matrix we cannot use the inverse matrix method."
      ],
      "metadata": {
        "id": "stazTg3bAKY7"
      }
    }
  ]
}